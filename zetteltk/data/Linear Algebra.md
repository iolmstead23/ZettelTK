**Linear Algebra** is a branch of [[mathematics]] that studies vectors, vector spaces, and linear transformations. It is the foundation for many areas of applied [[mathematics]], physics, [[computer]] science, engineering, and [[data]] science. The key focus is on solving systems of linear equations, understanding [[matrix]] operations, and exploring vector spaces and their properties.
#### **Key Concepts**

1. **[[vector|Vectors]]**:
    
    - Objects that have magnitude and direction.
    - Represented in $n$-dimensional space as ${v} = (v_1, v_2, \dots, v_n)$.
2. **Matrices**:
    
    - Rectangular arrays of numbers used to represent linear transformations and systems of equations.
    - A [[matrix]] $A$ of size $m \times n$ has $m$ rows and $n$ columns.
3. **Systems of Linear Equations**:
    
    - A collection of equations involving the same set of variables.
    - Represented compactly as $A\mathbf{x} = \mathbf{b}$, where $A$ is the coefficient [[matrix]], $\mathbf{x}$ is the variable [[vector]], and $\mathbf{b}$ is the result [[vector]].
4. **[[vector space|Vector Spaces]]**:
    
    - Collections of [[vector|vectors]] that satisfy certain properties (closure under addition and [[scalar]] multiplication).
    - Examples include [[euclidean geometry|Euclidean space]] $\mathbb{R}^n$ and [[function]] spaces.
5. **Linear Transformations**:
    
    - Functions that map vectors from one [[vector space]] to another, preserving [[vector]] addition and [[scalar]] multiplication.
    - Represented by matrices.
6. **[[Eigenvalues and Eigenvectors]]**:
    
    - For a [[matrix]] $A$, an eigenvector $\mathbf{v}$ satisfies $A\mathbf{v} = \lambda\mathbf{v}$, where $\lambda$ is the [[eigenvalues and eigenvectors|eigenvalue]].
    - Key for understanding [[matrix]] behavior, including rotations, scaling, and stability.
7. **Determinants**:
    
    - A [[scalar]] value associated with a square [[matrix]] that indicates whether the [[matrix]] is invertible.
    - Helps in solving systems of equations and understanding transformations.
8. **Orthogonality and Projections**:
    
    - Vectors are orthogonal if their dot product is zero.
    - Projections decompose a [[vector]] into components aligned and orthogonal to a given subspace.
#### **Brief History of Linear [[Algebra]]**

1. **Origins**:
    
    - Linear algebra originated with solving systems of linear equations in ancient civilizations such as Babylon (~2000 BCE) and China (~200 BCE).
2. **Development in the 17th Century**:
    
    - Ren√© Descartes introduced Cartesian coordinates, linking [[algebra]] and [[geometry]].
3. **19th Century Formalization**:
    
    - Augustin-Louis Cauchy and Arthur Cayley formalized matrices and determinants.
    - Hermann Grassmann developed the theory of [[vector]] spaces in 1844.
4. **Modern Era**:
    
    - In the 20th century, linear algebra became a cornerstone of applied [[mathematics]], particularly in quantum mechanics, [[statistics]], and [[computer]] science.
    - Tools such as [[Gaussian elimination]], eigenvalues, and singular value decomposition (SVD) became widely used.
#### **Applications of Linear Algebra**

1. **Physics**:
    
    - Modeling physical systems, quantum mechanics, and relativity.
2. **Engineering**:
    
    - Signal processing, control systems, and structural analysis.
3. **[[Computer Science]]**:
    
    - [[Computer]] graphics, [[machine learning]], [[data]] analysis, and cryptography.
4. **[[Mathematics]]**:
    
    - Solving [[differentiation|differential equations]], [[optimization]], and numerical analysis.
5. **Economics**:
    
    - Modeling complex systems, input-output analysis, and [[optimization]].